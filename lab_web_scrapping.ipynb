{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq7Fx0T_DPsE"
   },
   "source": [
    "# Web Scrapping lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3n7CJfCD2pK"
   },
   "source": [
    "In this lab you will scrappe this [website](https://books.toscrape.com/) of books.\n",
    "\n",
    "You have to create a Pandas DataFrame with all the books listed in the page. Each row of the DataFrame should contain information of each book. In particular, the DataFrmae must contain:\n",
    "\n",
    "* category\n",
    "* title\n",
    "* price\n",
    "* stock availability\n",
    "* star rating (number of stars)\n",
    "* description\n",
    "* UPC\n",
    "\n",
    "Happy scrapping!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaqLnJRLBqMS"
   },
   "source": [
    "# Server verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNXuRli_BvJp"
   },
   "source": [
    "Load the needed libraries, and make sure thar you can obtain the correct status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szKOZm99Frlf",
    "outputId": "184b2f54-7eaa-4d31-b66f-f3f8311e80e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting book information: 100%|██████████████| 20/20 [00:05<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data has been saved to 'books_data.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For showing progress bars\n",
    "\n",
    "# URL of the first page of the books\n",
    "url = 'http://books.toscrape.com/catalogue/category/books_1/index.html'\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all books on the page\n",
    "books = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Initialize a list to store book data\n",
    "book_data = []\n",
    "\n",
    "# Loop through each book and extract the required information\n",
    "for book in tqdm(books, desc=\"Extracting book information\"):\n",
    "    # Extract the title, price, stock availability, and rating\n",
    "    title = book.h3.a['title']\n",
    "    price = book.find('p', class_='price_color').text\n",
    "    stock = book.find('p', class_='availability').text.strip()\n",
    "    rating = book.p['class'][1]  # e.g., 'star-rating Three'\n",
    "    \n",
    "    # To get the description, category, and UPC, visit the book's detail page\n",
    "    book_url = book.h3.a['href']\n",
    "    book_page = requests.get(f'http://books.toscrape.com/catalogue/{book_url}')\n",
    "    book_soup = BeautifulSoup(book_page.text, 'html.parser')\n",
    "    \n",
    "    # Extract the description\n",
    "    description = book_soup.select_one('#product_description ~ p')\n",
    "    description = description.text.strip() if description else \"No description\"\n",
    "    \n",
    "    # Extract the category\n",
    "    breadcrumb = book_soup.select('ul.breadcrumb li a')\n",
    "    if len(breadcrumb) >= 3:\n",
    "        category = breadcrumb[-1].text.strip()\n",
    "    else:\n",
    "        category = \"No category\"\n",
    "    \n",
    "    # Extract the UPC\n",
    "    upc = None\n",
    "    upc_element = book_soup.find('th', string='UPC')\n",
    "    if upc_element:\n",
    "        upc = upc_element.find_next_sibling('td').text.strip()\n",
    "    \n",
    "    # Save all the information in a dictionary\n",
    "    book_data.append({\n",
    "        'Category': category,\n",
    "        'Title': title,\n",
    "        'Price': price,\n",
    "        'Availability': stock,\n",
    "        'Rating': rating,\n",
    "        'Description': description,\n",
    "        'UPC': upc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the collected book data\n",
    "books_df = pd.DataFrame(book_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "books_df.to_csv('books_data.csv', index=False)\n",
    "\n",
    "print(\"Scraping complete. Data has been saved to 'books_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful! Status code: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://books.toscrape.com/'\n",
    "\n",
    "# Send a GET request to the server to check the status\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code\n",
    "if response.status_code == 200:\n",
    "    print(\"Connection successful! Status code:\", response.status_code)\n",
    "else:\n",
    "    print(\"Failed to connect. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: \n",
      "    All products | Books to Scrape - Sandbox\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://books.toscrape.com/'\n",
    "\n",
    "# Send a GET request to the server and get the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Example: Print the title of the page to confirm successful parsing\n",
    "page_title = soup.title.text\n",
    "print(\"Page Title:\", page_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Star Rating</th>\n",
       "      <th>UPC</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>Â£51.77</td>\n",
       "      <td>In stock</td>\n",
       "      <td>3</td>\n",
       "      <td>a897fe39b1053632</td>\n",
       "      <td>It's hard to imagine a world without A Light i...</td>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>Â£53.74</td>\n",
       "      <td>In stock</td>\n",
       "      <td>1</td>\n",
       "      <td>90fa61229261140a</td>\n",
       "      <td>\"Erotic and absorbing...Written with starling ...</td>\n",
       "      <td>Historical Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>Â£50.10</td>\n",
       "      <td>In stock</td>\n",
       "      <td>1</td>\n",
       "      <td>6957f44c3847a760</td>\n",
       "      <td>Dans une France assez proche de la nÃ´tre, un ...</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>Â£47.82</td>\n",
       "      <td>In stock</td>\n",
       "      <td>4</td>\n",
       "      <td>e00eb4fd7b871a48</td>\n",
       "      <td>WICKED above her hipbone, GIRL across her hear...</td>\n",
       "      <td>Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>Â£54.23</td>\n",
       "      <td>In stock</td>\n",
       "      <td>5</td>\n",
       "      <td>4165285e1663650f</td>\n",
       "      <td>From a renowned historian comes a groundbreaki...</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title    Price Availability  Star Rating  \\\n",
       "0                   A Light in the Attic  Â£51.77     In stock            3   \n",
       "1                     Tipping the Velvet  Â£53.74     In stock            1   \n",
       "2                             Soumission  Â£50.10     In stock            1   \n",
       "3                          Sharp Objects  Â£47.82     In stock            4   \n",
       "4  Sapiens: A Brief History of Humankind  Â£54.23     In stock            5   \n",
       "\n",
       "                UPC                                        Description  \\\n",
       "0  a897fe39b1053632  It's hard to imagine a world without A Light i...   \n",
       "1  90fa61229261140a  \"Erotic and absorbing...Written with starling ...   \n",
       "2  6957f44c3847a760  Dans une France assez proche de la nÃ´tre, un ...   \n",
       "3  e00eb4fd7b871a48  WICKED above her hipbone, GIRL across her hear...   \n",
       "4  4165285e1663650f  From a renowned historian comes a groundbreaki...   \n",
       "\n",
       "             Category  \n",
       "0              Poetry  \n",
       "1  Historical Fiction  \n",
       "2             Fiction  \n",
       "3             Mystery  \n",
       "4             History  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List to store the information for each book\n",
    "books_data = []\n",
    "\n",
    "# Locate the section containing all books\n",
    "books_section = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Iterate over each book\n",
    "for book in books_section:\n",
    "    # Extract the title\n",
    "    title = book.h3.a['title']\n",
    "    \n",
    "    # Extract the price\n",
    "    price = book.find('p', class_='price_color').text\n",
    "    \n",
    "    # Extract the stock availability\n",
    "    availability = book.find('p', class_='instock availability').text.strip()\n",
    "    \n",
    "    # Extract the star rating (convert to a number)\n",
    "    rating_class = book.p['class']\n",
    "    star_rating = rating_class[-1]\n",
    "    star_rating = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}.get(star_rating, 0)\n",
    "    \n",
    "    # Navigate to the book's detail page for more information\n",
    "    book_url = url + book.h3.a['href']\n",
    "    book_response = requests.get(book_url)\n",
    "    book_soup = BeautifulSoup(book_response.text, 'html.parser')\n",
    "    \n",
    "    # Extract UPC\n",
    "    upc = book_soup.find('th', string='UPC').find_next_sibling('td').text\n",
    "    \n",
    "    # Extract Description\n",
    "    description = book_soup.find('meta', {'name': 'description'})['content'].strip()\n",
    "    \n",
    "    # Extract Category (assumes categories are listed in breadcrumb)\n",
    "    category = book_soup.find('ul', class_='breadcrumb').find_all('li')[2].text.strip()\n",
    "    \n",
    "    # Append the book's information to the list\n",
    "    books_data.append({\n",
    "        'Title': title,\n",
    "        'Price': price,\n",
    "        'Availability': availability,\n",
    "        'Star Rating': star_rating,\n",
    "        'UPC': upc,\n",
    "        'Description': description,\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "# Convert the list to a Pandas DataFrame\n",
    "books_df = pd.DataFrame(books_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "books_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'books_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "books_df.to_csv('books_data.csv', index=False)\n",
    "\n",
    "print(\"Data saved to 'books_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'books_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "books_df.to_excel('books_data.xlsx', index=False)\n",
    "\n",
    "print(\"Data saved to 'books_data.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'books_data.json'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a JSON file\n",
    "books_df.to_json('books_data.json', orient='records', lines=True)\n",
    "\n",
    "print(\"Data saved to 'books_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAq9izU8Cpyx"
   },
   "source": [
    "# Books in a given category\n",
    "\n",
    "Use. web scrapping and list comprehension to obtain the **absolute** url of each book to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "gC6uP76bHwNv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html',\n",
       " 'https://books.toscrape.com/catalogue/full-moon-over-noahs-ark-an-odyssey-to-mount-ararat-and-beyond_811/index.html',\n",
       " 'https://books.toscrape.com/catalogue/see-america-a-celebration-of-our-national-parks-treasured-sites_732/index.html',\n",
       " 'https://books.toscrape.com/catalogue/vagabonding-an-uncommon-guide-to-the-art-of-long-term-world-travel_552/index.html',\n",
       " 'https://books.toscrape.com/catalogue/under-the-tuscan-sun_504/index.html',\n",
       " 'https://books.toscrape.com/catalogue/a-summer-in-europe_458/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-great-railway-bazaar_446/index.html',\n",
       " 'https://books.toscrape.com/catalogue/a-year-in-provence-provence-1_421/index.html',\n",
       " 'https://books.toscrape.com/catalogue/the-road-to-little-dribbling-adventures-of-an-american-in-britain-notes-from-a-small-island-2_277/index.html',\n",
       " 'https://books.toscrape.com/catalogue/neither-here-nor-there-travels-in-europe_198/index.html',\n",
       " 'https://books.toscrape.com/catalogue/1000-places-to-see-before-you-die_1/index.html']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = 'https://books.toscrape.com/'\n",
    "\n",
    "# URL of the specific category (for example, \"Travel\")\n",
    "category_url = base_url + 'catalogue/category/books/travel_2/index.html'\n",
    "\n",
    "# Send a GET request to the category page\n",
    "response = requests.get(category_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the book links on the category page\n",
    "# The links are usually within 'h3' tags under 'article' with class 'product_pod'\n",
    "book_links = soup.find_all('h3')\n",
    "\n",
    "# Use list comprehension to create a list of absolute URLs\n",
    "book_urls = [base_url + link.find('a')['href'].replace('../../../', 'catalogue/') for link in book_links]\n",
    "\n",
    "# Display the list of absolute URLs\n",
    "book_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLIh0a_LDJMo"
   },
   "source": [
    "# Book details\n",
    "\n",
    "Create a Python function that given a book_url as an input returns a dictionary with the following structure:\n",
    "\n",
    "```Python\n",
    "{\"Title\": title, \"Price\": price, \"Availability\": availability, \"Rating\": rating, \"Description\": description, \"UPC\": upc}\n",
    "```\n",
    "\n",
    "where `description` should contain the book's summary given in the Product description, and the values are the book's associated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "5-uC7hz1LxCd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'A Light in the Attic', 'Price': 'Â£51.77', 'Availability': 'In stock (22 available)', 'Rating': 3, 'Description': \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\", 'UPC': 'a897fe39b1053632'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_book_details(book_url):\n",
    "    # Send a GET request to the book's detail page\n",
    "    response = requests.get(book_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the title\n",
    "    title = soup.find('h1').text\n",
    "    \n",
    "    # Extract the price\n",
    "    price = soup.find('p', class_='price_color').text\n",
    "    \n",
    "    # Extract the stock availability\n",
    "    availability = soup.find('p', class_='instock availability').text.strip()\n",
    "    \n",
    "    # Extract the star rating (convert to a number)\n",
    "    rating_class = soup.find('p', class_='star-rating')['class']\n",
    "    rating = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}.get(rating_class[-1], 0)\n",
    "    \n",
    "    # Extract UPC\n",
    "    upc = soup.find('th', string='UPC').find_next_sibling('td').text\n",
    "    \n",
    "    # Extract Description (check if it exists, as some books might not have a description)\n",
    "    description_tag = soup.find('meta', {'name': 'description'})\n",
    "    description = description_tag['content'].strip() if description_tag else 'No description available'\n",
    "    \n",
    "    # Return the details as a dictionary\n",
    "    book_details = {\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Availability\": availability,\n",
    "        \"Rating\": rating,\n",
    "        \"Description\": description,\n",
    "        \"UPC\": upc\n",
    "    }\n",
    "    \n",
    "    return book_details\n",
    "\n",
    "# Example usage\n",
    "book_url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'  # Example book URL\n",
    "book_info = get_book_details(book_url)\n",
    "print(book_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvSqhX4UDxbb"
   },
   "source": [
    "# Collect and store all the information from the books in a Pandas DataFrame\n",
    "\n",
    "Start with the following dictionary:\n",
    "\n",
    "```python\n",
    "books_dict = {\"Title\": [], \"Price\": [], \"Availability\": [], \"Rating\": [], \"Description\": [], \"UPC\": [], \"Category\": [] }\n",
    "```\n",
    "\n",
    "Then, iterate over all the categories and all the books in a given category to collect any book information using the previous function. Fill the previous dictionary with the information about each book.\n",
    "\n",
    "Show the first five rows of the previous final Pandas DataFrame.\n",
    "\n",
    "Tip: You can use the function `tqdm` from the library `tqdm` to show a progress bar if in iterable of a for loop as shown below :wink: :\n",
    "\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "\n",
    "for elem in tqdm(iterable):\n",
    "    # some code\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "apeImoCxStA5",
    "outputId": "20d2d37c-5cbe-4062-e407-80f77955a8d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|████                                        | 1/11 [00:00<00:04,  2.34it/s]\u001b[A\n",
      " 18%|████████                                    | 2/11 [00:00<00:04,  2.04it/s]\u001b[A\n",
      " 27%|████████████                                | 3/11 [00:01<00:03,  2.17it/s]\u001b[A\n",
      " 36%|████████████████                            | 4/11 [00:01<00:03,  2.00it/s]\u001b[A\n",
      " 45%|████████████████████                        | 5/11 [00:02<00:02,  2.08it/s]\u001b[A\n",
      " 55%|████████████████████████                    | 6/11 [00:02<00:02,  1.95it/s]\u001b[A\n",
      " 64%|████████████████████████████                | 7/11 [00:03<00:02,  1.99it/s]\u001b[A\n",
      " 73%|████████████████████████████████            | 8/11 [00:03<00:01,  1.97it/s]\u001b[A\n",
      " 82%|████████████████████████████████████        | 9/11 [00:04<00:00,  2.02it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████    | 10/11 [00:05<00:00,  1.94it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 11/11 [00:05<00:00,  2.03it/s]\u001b[A\n",
      "  2%|▉                                           | 1/50 [00:05<04:53,  6.00s/it]\u001b[A\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:08,  2.31it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.23it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [00:01<00:07,  2.21it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [00:01<00:07,  2.25it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [00:02<00:06,  2.25it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [00:02<00:06,  2.22it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [00:03<00:05,  2.24it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [00:03<00:05,  2.26it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [00:04<00:04,  2.26it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [00:04<00:04,  2.23it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [00:04<00:04,  2.18it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [00:05<00:03,  2.10it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [00:05<00:03,  2.11it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [00:06<00:02,  2.16it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [00:06<00:02,  2.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [00:07<00:01,  2.20it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [00:07<00:01,  2.14it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [00:08<00:00,  2.17it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [00:08<00:00,  2.18it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [00:09<00:00,  2.18it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:15<06:32,  8.17s/it]\u001b[A\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:00<00:08,  2.21it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [00:00<00:08,  2.16it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to get book details (from previous step)\n",
    "def get_book_details(book_url):\n",
    "    response = requests.get(book_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    title = soup.find('h1').text\n",
    "    price = soup.find('p', class_='price_color').text\n",
    "    availability = soup.find('p', class_='instock availability').text.strip()\n",
    "    \n",
    "    rating_class = soup.find('p', class_='star-rating')['class']\n",
    "    rating = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}.get(rating_class[-1], 0)\n",
    "    \n",
    "    upc = soup.find('th', string='UPC').find_next_sibling('td').text\n",
    "    \n",
    "    description_tag = soup.find('meta', {'name': 'description'})\n",
    "    description = description_tag['content'].strip() if description_tag else 'No description available'\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Price\": price,\n",
    "        \"Availability\": availability,\n",
    "        \"Rating\": rating,\n",
    "        \"Description\": description,\n",
    "        \"UPC\": upc\n",
    "    }\n",
    "\n",
    "# Initialize the dictionary to store the information\n",
    "books_dict = {\"Title\": [], \"Price\": [], \"Availability\": [], \"Rating\": [], \"Description\": [], \"UPC\": [], \"Category\": [] }\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = 'https://books.toscrape.com/'\n",
    "\n",
    "# Send a GET request to the main page\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all categories\n",
    "categories = soup.find('ul', class_='nav nav-list').find('ul').find_all('li')\n",
    "\n",
    "# Iterate over each category\n",
    "for category in tqdm(categories):\n",
    "    # Get the category name and URL\n",
    "    category_name = category.a.text.strip()\n",
    "    category_url = base_url + category.a['href']\n",
    "    \n",
    "    # Send a GET request to the category page\n",
    "    response = requests.get(category_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all the book links on the category page\n",
    "    book_links = soup.find_all('h3')\n",
    "    book_urls = [base_url + link.find('a')['href'].replace('../../../', 'catalogue/') for link in book_links]\n",
    "    \n",
    "    # Iterate over each book in the category\n",
    "    for book_url in tqdm(book_urls, leave=False):\n",
    "        # Get book details using the previously defined function\n",
    "        book_info = get_book_details(book_url)\n",
    "        \n",
    "        # Add the book's information to the dictionary\n",
    "        books_dict[\"Title\"].append(book_info[\"Title\"])\n",
    "        books_dict[\"Price\"].append(book_info[\"Price\"])\n",
    "        books_dict[\"Availability\"].append(book_info[\"Availability\"])\n",
    "        books_dict[\"Rating\"].append(book_info[\"Rating\"])\n",
    "        books_dict[\"Description\"].append(book_info[\"Description\"])\n",
    "        books_dict[\"UPC\"].append(book_info[\"UPC\"])\n",
    "        books_dict[\"Category\"].append(category_name)\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "books_df = pd.DataFrame(books_dict)\n",
    "\n",
    "# Display the first five rows of the DataFrame\n",
    "books_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
